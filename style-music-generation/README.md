# Style-music-generation

A cute multi-layer LSTM network that can perform like a human ğŸ¶ It learns the dynamics of music! The architecture was specifically designed to handle music of different genres.

I just re-implemented the author's work, then please read his [blog post](http://imanmalik.com/cs/2017/06/05/neural-style.html) and paper:

> **Iman Malik, Carl Henrik Ek, [*"Neural Translation of Musical Style"*](https://arxiv.org/abs/1708.03535), 2017.**


## å‡†å¤‡å·¥ä½œ

æ‰€éœ€è¦çš„ç¯å¢ƒï¼špython 2.7 & Tensorflow

python package

1. midi2audio
2. mido
3. pretty_midi
è¿™ä¸‰ä¸ªéƒ½å¾ˆå®¹æ˜“å®‰è£…

## è¿è¡Œ

è¯·è¿è¡Œmusic.ipynbæ–‡ä»¶ï¼Œè¿™æ˜¯æˆ‘æµ‹è¯•è¿è¡ŒæˆåŠŸçš„æ–‡ä»¶

## The Piano Dataset
I created my own dataset for the model. If you wish to use the Piano Dataset ğŸ¹ for academic purposes, you can download it from [here.](http://imanmalik.com/assets/dataset/TPD.zip) The Piano Dataset is distributed with a [CC-BY 4.0 license](https://creativecommons.org/licenses/by/4.0/). If you use this dataset, please reference this [paper](https://arxiv.org/abs/1708.03535):

## æ–‡ä»¶(éŸ³ä¹æ–‡ä»¶å¤ªå¤§äº†å°±è‡ªå·±ä¸‹å¥½äº†ä¸æ”¾ä¸Šæ¥äº†)
`\run` : ä¿å­˜äº†è¿è¡Œç»“æœ.

`\data` : åŒ…æ‹¬inputså’Œvelocities, æœ€åˆçš„å¯¹éŸ³ä¹æ–‡ä»¶çš„å­˜å‚¨.

`\predict` : ç»“æœæ–‡ä»¶ï¼Œoriginæ˜¯æµ‹è¯•éŸ³é¢‘, original_classicalå’Œoriginal_jazzéƒ½æ˜¯ç”Ÿæˆçš„ä¸åŒé£æ ¼çš„éŸ³ä¹.

`main.py` : the main file to implemented.
`convert-format.rb` : This was used to convert format 1 MIDIs into format 0.  
`file_util.py` : This contains folder/file-handling functions.  
`midi_util.py` : This contains MIDI-handling functions.  
`model.py` : StyleNet's Class.  
`data_util.py` : For shuffling and batching data during training.  

